---
title: "Linear and Logistic Regression of Poker Database"
Author: "Justin Herman"
output:
  html_document:
    theme: "simplex"
    highlight: 'pygments'
    toc: true
    toc_float: true
    

---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


# Research question
+ Develop a model using linear regression to predict win rates in poker. I play online poker professionally.  What led me into the topic actually has to do with my data 607 final. Here is the link to that project. [Data 607 Final](https://rpubs.com/justin_herman_42/385739).  
<br />  
The short explanation is that I had to access my poker Postgres database, create a customized string of poker statistics, and automate the insertion of that string into my poker sites note file.  After querying the database and creating custom stats, I realized I wanted to attempt to fit a model to the data. While I doubt anything I discover will be a breakthrough, I would think poker players could find it useful.  
    
# Data 
+ Every hand I play in poker is tracked in text format.  It is then converted behind the scenes by software into statistics describing player actions. How often someone takes an action as well as the opportunity to take that action are recorded.  This  allows me to create percentages of actions, represented as poker statistics.  These stats are stored inside a Postgres database.  For this project I am accessing that db.  
<br />
The cases in this study are the players in the database.  Our target variable is BB/100(win rate).  The independent variables are quantitative (vpip, pfr, wwsf, threebet) and qualitative (vpip-pfr split into a qualitative grouping of wide and narrow gap).  See "explanation of stats" section below, taken from data 607 project, to understand what these variables represent.  This is an observational study. The population of interest is online poker players.  My data comes from multiple sites I have played on in past couple years, therefore it is the global online poker player population.  

# Generalizability 
+ Generalizability from my stats is a difficult task.  Most of my stats come from tables I play at.  Tables aren't chosen at random.  I use careful table selection to select tables where worse players play. This likely biases the player pool. Playing style and strategy at different stakes can lead to different results.  It's complicated, but poker is about capitalizing on mistakes. Different types of mistakes are likely made at different levels.  For instance, when playing a free hand of poker people play much differently than they would if they had to invest substantial money. Perhaps if the population is narrowly defined as low to mid stakes online No Limit Holdem, some of the predictions can be generalized to the population. 


# Explanation of Stats
+ As this is the only poker technical area in this project, I provide a brief explanation of some poker stats. In Texas Holdem players are all given two cards and are presented with a betting decision based on only their individual cards. From there they are presented with decisions on what to do as 5 community cards come out over three more rounds of betting.  There are thousands of combinations of hands and hundreds of stats to choose from, but the stats I chose are the following: 
    + VPIP = How often someone calls their hand `Or` raises/ total hands played
        + Ideal range for this stat is from (22-28)
    + PFR = how often someone raises their hand / /total hands played 
        + Ideal range for this stat is from (16-22)
    + VPIP includes the entire set of PFR 
    + VPIP_PFR = VPIP-PFR
    + WWSF = Percent of the time someone wins hand after seeing a flop
    + Threebet = After someone has already raised, the percent of the time you re-raise
    + BB/100 = how many bets a player wins per 100 hands(how much someone wins)
        + Typically any win rate above 4/bb 100 is considered a solid winning player
        + This stat can be both positive and negative, negative represents losing players

# Big Picture Overview

+ Query my poker Postgres database and create customized player statistics
+ Explore these statistics and test to see if assumptions for inference are met
+ Use these statistics to run a multiple linear regression model to try and predict a playerâ€™s winrate


# Setup Access To Postgres, Load Libraries
```{r,echo=FALSE,message=FALSE,warning=FALSE}
rm(list = ls())
library(formatR)
library(gridExtra)
library(XML)
require("RPostgreSQL")
library(tidyverse)
library(kableExtra)
library(knitr)
library(psych)
library(stats)

pw <- "password"
drv <- dbDriver("PostgreSQL")
con <- dbConnect(drv, dbname = "2017_DB", host = "localhost", port = 5432, 
    user = "postgres", password = pw)


```


## Test connection  

```{r,echo=TRUE}
tryCatch({
    drv <- dbDriver("PostgreSQL")
    print("Connecting to database")
    conn <- con
    print("Connected!")
}, error = function(cond)
{
    print("Unable to connect to database.")
})
```

# Explore Postgres DB
```{r}
# query 6 has sn's
query_6 <- dbGetQuery(conn, "SELECT * FROM players")
# query 7 has stats
query_7 <- dbGetQuery(conn, "SELECT * FROM compiledplayerresults limit 1000000")
```

## Filter For Desired Columns 
```{r}
# Combine query 6 and 7 A
all_players <- query_6 %>% 
    select(., c(playername, player_id, cashhands))
player_ids <- all_players$player_id
all_player_stats <- query_7 %>% 
    filter(., player_id %in% player_ids)
all_players_db <- merge(all_player_stats, all_players, by = "player_id", 
    all = TRUE)

# Choosen columns
columns_for_analysis <- c("gametype_id", "player_id", "totalhands", "totalbbswon", 
    "totalrakeincents", "totalamountwonincents", "vpiphands", "pfrhands", 
    "couldcoldcall", "didcoldcall", "couldthreebet", "didthreebet", "couldsqueeze", 
    "didsqueeze", "facingtwopreflopraisers", "calledtwopreflopraisers", 
    "raisedtwopreflopraisers", "smallblindstealattempted", "smallblindstealdefended", 
    "smallblindstealreraised", "bigblindstealattempted", "bigblindstealdefended", 
    "bigblindstealreraised", "facedthreebetpreflop", "foldedtothreebetpreflop", 
    "calledthreebetpreflop", "raisedthreebetpreflop", "facedfourbetpreflop", 
    "foldedtofourbetpreflop", "calledfourbetpreflop", "raisedfourbetpreflop", 
    "sawflop", "wonhandwhensawflop", "sawshowdown", "wonshowdown", "flopcontinuationbetpossible", 
    "flopcontinuationbetmade", "turncontinuationbetpossible", "turncontinuationbetmade", 
    "rivercontinuationbetpossible", "rivercontinuationbetmade", "facingflopcontinuationbet", 
    "foldedtoflopcontinuationbet", "calledflopcontinuationbet", "raisedflopcontinuationbet", 
    "facingturncontinuationbet", "foldedtoturncontinuationbet", "calledturncontinuationbet", 
    "raisedturncontinuationbet", "facingrivercontinuationbet", "foldedtorivercontinuationbet", 
    "calledrivercontinuationbet", "raisedrivercontinuationbet", "playername", 
    "cashhands")
         

```

## Create Final DF
+ Filter by desired stats
+ Group_by to aggregate by player
+ Build desired stats 
```{r}

predictors <- c("vpip", "pfr", "threebet", "bb_per_100", "wwsf")
indexes <- c((1:31), 65)
# Create db with custom stats
all_players_finished_db <- all_players_db %>% 
    filter(., gametype_id %in% indexes) %>% 
    select(c(columns_for_analysis)) %>% 
    group_by(playername) %>% 
    summarize(vpip = round(sum(vpiphands)/sum(totalhands) * 100, 1), 
       pfr = round(sum(pfrhands)/sum(totalhands) * 100, 1), 
       total_hands = sum(totalhands),
       money_won = sum(totalamountwonincents), 
       rake = sum(totalrakeincents), 
       threebet = round(sum(didthreebet)/sum(couldthreebet) * 100, 1), 
       bb_per_100 = round(sum(totalbbswon)/(sum(totalhands)), 2), 
       total_rake_100 = round((sum(totalrakeincents)/100)/sum(total_hands), 2), 
       money_won_100 = round((sum(totalamountwonincents)/100)/(sum(total_hands)/100)/100, 2), 
       wwsf = round(sum(wonhandwhensawflop)/sum(sawflop) * 100, 1))
# Print out of my statistics
my_stats <- all_players_finished_db %>% filter(playername == "RileyFreeman")
kable(my_stats)
```                 


# Data Exploration 

## Determine Proper Hand Count For  Observations
+ Some stats can take thousands of hands to normalize
+ I want to explore a dataset where my stats are normalized,however, doing so means that I lose total observations to look at.    
+ Therefore, Below I filter my db by amount of hands played.  I then plot the distributions of the player stats.  The filters are as following
    + All players, over 50 hands played, over 100 hands played, over 500 hands played and over 2500 hands played

```{r,echo=FALSE,message=FALSE}

# Load csv from Github, allow for reproducibility
csv_link <- paste("https://raw.githubusercontent.com/justinherman42/",
                  "Justin-Data-607/master/final%20project/data606finaldb.csv",
                  sep="")
all_players_finished_db <- read.csv(csv_link)

## create list of predictors,
predictors <- c("vpip", "pfr", "threebet", "bb_per_100", "wwsf")

# Created DF by hand filters w list of predictors

all_hands <- all_players_finished_db %>% 
    select(., c(predictors))
over_50_hands <- all_players_finished_db %>% 
    filter(., total_hands > 50) %>% 
    select(., c(predictors))
over_100_hands <- all_players_finished_db %>% 
    filter(., total_hands > 100) %>% 
    select(., c(predictors))
over_500_hands <- all_players_finished_db %>% 
    filter(., total_hands > 500) %>% 
    select(., c(predictors))
over_1000_hands <- all_players_finished_db %>% 
    filter(., total_hands > 
    1000) %>% select(., c(predictors))
over_2500_hands <- all_players_finished_db %>% 
    filter(., total_hands > 
    2500) %>% select(., c(predictors))
```



```{r}
## Code below was neat and clean, so i made it viewable
## Create a list of df's, and a list of df names 
## seq along these lists for data visualization

db_names <- list(all_hands,over_50_hands,over_100_hands,over_500_hands,over_1000_hands,over_2500_hands)

titles <- list("all_hands","over_50_hands","over_100_hands","over_500_hands","over_1000_hands","over_2500_hands")
for (i in seq_along(db_names)){
    p <- lapply(predictors,function(x){ggplot(db_names[[i]],
            aes_string(x = x)) +
            geom_histogram(binwidth = 3,col="red",aes(fill=..count..))+
            labs(title= titles[i])+
             scale_x_continuous()})
    do.call(grid.arrange,  p)
}

```



# Observations 

## Closer Look At Dataframe With No Hands Played Filter 
+ Looking at the no hands filtered db, the distributions are all over the place.  It is heavily zero inflated 
    + WWSF- seems to have alot of 100 and 0 frequency scores.  This makes sense as nearly 23k players have played less than 50 hands. The sample size effectively prevents the data set from displaying as a true continuous variable.  
+ Let's take a closer look at what the under 50 hands played distributions look like below

## Under 50 Hands
```{r,echo= FALSE,warning=FALSE,message=FALSE}
under_50_hands <- all_players_finished_db %>% 
    filter(., total_hands < 50)
par(mfrow = c(2, 3))
mapply(hist, under_50_hands[, predictors], main = colnames(under_50_hands[, 
    predictors]), xlab = " Under50 hands")
mtext("Figure 3", SOUTH <- 1, line = 3, adj = 3, col = "blue")
```
   
    
+ 0 becomes the mode in the under 50 hands played for WWSF, 
+ 50 is the second highest value, and 100 is the third highest value.  
+ WWSF describes how often someone wins a hand, given that they saw a flop.  It is very unlikely, given a larger sample size, that a player would win/lose every hand when they saw the flop or play exactly an even amount of hands and win every other one; yet these outcomes make up nearly 39% of the WWSF results in the under 50 hands DF. Math below 
  
```{r,echo= FALSE,warning=FALSE,message=FALSE}
wwsf_50 <- all_players_finished_db %>% 
    filter(., total_hands < 50) %>% 
    select(wwsf) %>% filter(., wwsf == 50)
wwsf_0 <- all_players_finished_db %>% 
    filter(., total_hands < 50) %>% 
    select(wwsf) %>% 
    filter(., wwsf == 0)
wwsf_100 <- all_players_finished_db %>% 
    filter(., total_hands < 50) %>% 
    select(wwsf) %>% filter(., wwsf == 100)

kable(paste("these 3 values(0,50,100) make up ", round((4462 + 2618 + 1897)/23183*100, 
    2), "% of total occurences"))

```
  
  
+ Confidence intervals for the WWSF stat would be interesting, but it doesn't fall within the point of my analysis.  I am trying to ballpark when my dataset variables become continuous.  Given the logical assumptions above, the under_50 sample isn't large enough to run  analysis on.  
    +  Increase my filter by hand requirement to allow for the stats to become continuous. 
+  Normality of my input variables isn't a requirement,however, I want to monitor how the hand filter effects the distributions

## Comparing 50 and 100 Hand Filters With Describe
+ The over 50 and over 100 hands Dataframes are displayed together above in figure 2
    + Much more normalized distribution across the board of predictor stats
        + WWSF has normalized and  the variables are likely continuous 
+ Run describe function over these two DF


```{r,echo=FALSE}

describe_over_50_hands <- describe(over_50_hands)
# Build describe> 50 hands played db
df_describe_50_hands <- as_data_frame(lapply(describe_over_50_hands, function(x)
{
    if (is.numeric(x)) 
        round(x, 2) else x
}))
colnames(df_describe_50_hands) <- colnames(describe_over_50_hands)
rownames(df_describe_50_hands) <- rownames(describe_over_50_hands)
# Display
kable(df_describe_50_hands, caption = "Figure 4 - Over 50 hands")
# Build describe >100 hands played db
describe_over_100_hands <- describe(over_100_hands)
describe_over_100_hands <- describe_over_100_hands[predictors, ]
df_describe_100_hands <- as_data_frame(lapply(describe_over_100_hands, 
    function(x)
    {
        if (is.numeric(x)) 
            round(x, 1) else x
    }))
colnames(df_describe_100_hands) <- colnames(describe_over_100_hands)
rownames(df_describe_100_hands) <- rownames(describe_over_100_hands)
# Display
kable(df_describe_100_hands, caption = "Figure 5 - Over 100")
```
 
     
+ The stats seem to fit each other very well

## Run Describe Function Over Other Filtered DF's 
 
```{r, echo=FALSE}
# Build over 500 played DB
describe_over_500_hands <- describe(over_500_hands)
describe_over_500_hands <- describe_over_500_hands[predictors, ]
df_describe_500_hands <- as_data_frame(lapply(describe_over_500_hands, 
    function(x)
    {
        if (is.numeric(x)) 
            round(x, 1) else x
    }))
colnames(df_describe_500_hands) <- colnames(describe_over_500_hands)
rownames(df_describe_500_hands) <- rownames(describe_over_500_hands)
kable(df_describe_500_hands, caption = "Figure 6 - Over 500")

# Build over 1000 hands played db 
describe_over_1000_hands <- describe(over_1000_hands)
describe_over_1000_hands <- describe_over_1000_hands[predictors, ]
df_describe_1000_hands <- as_data_frame(lapply(describe_over_1000_hands, 
    function(x)
    {
        if (is.numeric(x)) 
            round(x, 1) else x
    }))
colnames(df_describe_1000_hands) <- colnames(describe_over_1000_hands)
rownames(df_describe_1000_hands) <- rownames(describe_over_1000_hands)
kable(df_describe_1000_hands, caption = "Figure 7 - Over 1000")

# Build over 2500 hands played db
describe_over_2500_hands <- describe(over_2500_hands)
describe_over_2500_hands <- describe_over_2500_hands[predictors, ]
df_describe_2500_hands <- as_data_frame(lapply(describe_over_2500_hands, 
    function(x)
    {
        if (is.numeric(x)) 
            round(x, 1) else x
    }))
colnames(df_describe_2500_hands) <- colnames(describe_over_2500_hands)
rownames(df_describe_2500_hands) <- rownames(describe_over_2500_hands)
kable(df_describe_2500_hands, caption = "Figure 8 - Over 2500")
 
```
 
 
 
## Run Some Normality QQ Plots 
 
```{r,echo= FALSE,warning=FALSE,message=FALSE}

par(mfrow = c(2, 3))
mapply(qqnorm, over_2500_hands[, predictors], main = colnames(over_50_hands[, 
    predictors]), xlab = "Figure 9 -over50 hands")

par(mfrow = c(2, 3))
mapply(qqnorm, over_100_hands[, predictors], main = colnames(over_100_hands[, 
    predictors]), xlab = "Figure 10 - Over100 hands")


par(mfrow = c(2, 3))
mapply(qqnorm, over_500_hands[, predictors], main = colnames(over_2500_hands[, 
    predictors]), xlab = "Figure 11 - Over500 hands")

par(mfrow = c(2, 3))
mapply(qqnorm, over_500_hands[, predictors], main = colnames(over_1000_hands[, 
    predictors]), xlab = "Figure 12 - Over1000 hands")

par(mfrow = c(2, 3))
mapply(qqnorm, over_2500_hands[, predictors], main = colnames(over_2500_hands[, 
    predictors]), xlab = "Figure 13 - Over2500 hands")
```

## Summary Statistics Meaning
+ While I ran descriptive statistics over the entire dataset, the main area of concern is the dependent variable, that variable is BB/100
+  QQplots of BB/100 are not very comforting in any of the Dataframes.  There appears to be many samples that fall outside  2,3,4 SD from the mean
+  Histograms also seem to have large tails and don't appear normal
+ With this in mind and with this being a elementary level analysis, I will proceed and attempt to run some linear models 


# Attempt to build Linear Models
+ Each model is run
    + Residual plots are graphed(although largely ignored until model is tuned)
    + Summary and Anova results are displayed

## Attempt 1
+ Filter for over 2500 hands
    + This Dataframe will consist of players who play rather often, the term for this in poker is "regs"
+ I will create one categorical input known as vpip-pfr.
    + If you refer to the [Data 607 Final](https://rpubs.com/justin_herman_42/385739), I ran some summary statistics on groupings of this stat in the section "do stats really matter" 
    + Below code is run to create a wide_gap and narrow_gap vpip-pfr column
    
    
```{r}
over_2500_hands <- all_players_finished_db %>% 
    filter(., total_hands > 2500) %>% 
    select(., c(predictors))
## Create numeric column
over_2500_hands <- over_2500_hands %>% mutate(., vpip_pfr = vpip - pfr)
# Save this vector for use later
numerical_vpip_pfr <- over_2500_hands$vpip_pfr
# Create categorical factor column
over_2500_hands$vpip_pfr[over_2500_hands$vpip_pfr < 15.001] <- 1
over_2500_hands$vpip_pfr[over_2500_hands$vpip_pfr > 15.001] <- 0
my_vector <- str_replace(as.character(over_2500_hands$vpip_pfr), "0", "wide_gap")
my_vector <- str_replace(my_vector, "1", "narrow_gap")
over_2500_hands$vpip_pfr <- as_factor(my_vector)
# Display new df summary stats
summary(over_2500_hands)
```

### Create LM model(Fit_1)
+ VPIP and PFR may violate independence between variables assumption as they likely influence the new vpip_pfr category 
    + I Proceed anyway
+ Target=BB/100(winrate)
+ Input variables- categorical-vpip_pfr, numerical-WWSF,VPIP,PFR,THREE_BET 

```{r}
y <- over_2500_hands$bb_per_100
vpip_pfr <- over_2500_hands$vpip_pfr
WWSF <- over_2500_hands$wwsf
VPIP <- over_2500_hands$vpip
PFR <- over_2500_hands$pfr
THREE_BET <- over_2500_hands$threebet
fit_1 <- lm(y ~ vpip_pfr + VPIP + THREE_BET + WWSF + PFR)
layout(matrix(c(1, 2, 3, 4), 2, 2))
plot(fit_1)
summary(fit_1)
anova(fit_1)
```

### Summary of Fit_1
+ Narrow gap is worth 2.8 bb( it's p value appears to show it's not significant)
+ WWSF also appears to not reach significance
    + Remove WWSF and proceed from there

## Fit_2

+ Removes WWSF
    
```{r}
fit_2 <- lm(y ~ vpip_pfr + VPIP + THREE_BET + PFR)

layout(matrix(c(1, 2, 3, 4), 2, 2))
plot(fit_2)
summary(fit_2)
anova(fit_2)
```

### Summary of Fit 2
 +  Model still shows that  categorical data is likely sharing colinearity with vpip and pfr, which makes sense.
    + Look at the correlations of the inputs 
        + Add vpip_pfr numeric column

## Correlation Plot
```{r}
library(corrplot)
# Add vpip-pfr numerical vector
corr_plot_db <- as_data_frame(cbind(over_2500_hands, numerical_vpip_pfr))
# Plot correlations

corrplot(cor(corr_plot_db[, -6]))
```
 
 
### Summary of Correlation Plot
 + As I Expected numerical vpip_pfr heavily correlates with vpip and pfr, it's also largely negatively correlated with the win rate statistic(bb_per_100)
 + Remove vpip( has the highest correlation with vpip_pfr)
 
## Fit_3
 
```{r}
fit_3 <- lm(y ~ vpip_pfr + THREE_BET + PFR)

layout(matrix(c(1, 2, 3, 4), 2, 2))
plot(fit_3)
my_fit <- summary(fit_3)
anova(fit_3)
```
 
### Summary of Fit 3

+ These results are interesting and I believe they are getting much closer
+ Just for refresher, below are the summary statistics on this Dataframe

```{r,echo=FALSE}
describe_over_2500_hands <- describe(over_2500_hands)
describe_over_2500_hands <- describe_over_2500_hands[predictors, ]
df_describe_2500_hands <- as_data_frame(lapply(describe_over_2500_hands, 
    function(x)
    {
        if (is.numeric(x)) 
            round(x, 2) else x
    }))
colnames(df_describe_2500_hands) <- colnames(describe_over_2500_hands)
rownames(df_describe_2500_hands) <- rownames(describe_over_2500_hands)
kable(df_describe_2500_hands, caption = "Figure 8 - Over 2500")
```



+ bb/100  mean is -12 and median is around -8
+ The intercept in the first model was at 14, it's now  at -15. After the coefficients, this seems to fit the data better 
    +  Categorical grouping of narrow_gap, is now worth 24.38 bb 
+ Most of the variance is also being explained by the categorical data, although the overall adjusted r^2 does seem worse in the third model than it was in the first model
+  Residuals show a large tail in QQ plot, and several outliers
    + I don't believe outliers can be addressed as they are real samples 

### Compare Fit_3 to my winrate
```{r}
my_stats <- all_players_finished_db %>% filter(playername == "RileyFreeman") %>% 
    select(., predictors)
for_comparison <- fit_3$coefficients[1] + fit_3$coefficients[2] + my_stats$pfr * 
    (fit_3$coefficients[4]) + my_stats$threebet * (fit_3$coefficients[3])
# Print prediction versus actual
paste("my actual win rate is", my_stats$bb_per_100, "model predicts ", 
    for_comparison)

```
+  A problem with the model is that the relationship of 3 bet can't really be summed up in a linear way, as there are inflection points.  
    + High 3 bets and low 3 bets are bad, I will attempt to square the threebet input to see if the model works better

## Fit_4

+ Square threebet input

```{r,echo=FALSE}
THREE_BET_2 <- THREE_BET^2
fit_4 <- lm(y ~ vpip_pfr + THREE_BET_2 + PFR)
# Graphs for analysis
layout(matrix(c(1, 2, 3, 4), 2, 2))
plot(fit_4)
summary(fit_4)
anova(fit_4)

for_comparison <- fit_4$coefficients[1] + fit_4$coefficients[2] + my_stats$pfr * 
    (fit_4$coefficients[4]) + my_stats$threebet * (fit_4$coefficients[3])
# Print prediction versus actual
paste("my actual win rate is", my_stats$bb_per_100, "model predicts ", 
    for_comparison)
```

+ This model is starting to look better, let me now try to square pfr as well

## Fit 5 
+  Square the pfr as well as threebet

```{r,echo=FALSE}
THREE_BET_2 <- THREE_BET^2
PFR_2 <- PFR^2
fit_5 <- lm(y ~ vpip_pfr + THREE_BET_2 + PFR_2)
# Graphs for analysis
layout(matrix(c(1, 2, 3, 4), 2, 2))
plot(fit_5)
summary(fit_5)
anova(fit_5)

for_comparison <- fit_5$coefficients[1] + fit_5$coefficients[2] + my_stats$pfr * 
    (fit_5$coefficients[4]) + my_stats$threebet * (fit_5$coefficients[3])
# Print prediction versus actual
paste("my actual win rate is", my_stats$bb_per_100, "model predicts ", 
    for_comparison)

```

+ Perhaps this is due to overfitting, but this looks pretty close.  Run a test train split and see what happens

### Test/Train split on Fit_5

```{r}
# Create test/train
set.seed(10)
train.idx <- sample(nrow(over_2500_hands), 0.7 * nrow(over_2500_hands), 
    replace = FALSE)
test.idx <- (1:nrow(over_2500_hands))[-train.idx]

# Extract target vector and rest of DF for LM
lm_target <- over_2500_hands[, "bb_per_100"]
lm_inputs <- over_2500_hands[, c("pfr", "threebet", "vpip_pfr")]
train_df <- over_2500_hands[train.idx, c("pfr", "threebet", "vpip_pfr", 
    "bb_per_100")]
test_df <- over_2500_hands[test.idx, c("pfr", "threebet", "vpip_pfr", "bb_per_100")]
THREE_BET_2 <- THREE_BET^2
PFR_2 <- PFR^2
fit_6 <- lm(bb_per_100 ~ vpip_pfr + (threebet^2) + (pfr^2), data = train_df)
fit_6 <- predict(fit_6, test_df)
actual <- over_2500_hands[test.idx, c("bb_per_100")]
error <- actual - fit_6
paste("my RMSE is", sqrt(mean(error^2)))
```

### Summary Fit 5
+  RMSE results are terrible
    + Missing a winrate by over 17 bb makes this model useless
    + It looks like the categorical data is doing all the heavy lifting 
        + The model is evaluating players into essentially two different strata, and the other dependent variables aren't really doing much

# Change of Direction
+ Luckily for me, I have a friend who is a data scientist and also happened to be a professional poker player.  He presented an idea to me
  + bb/100 is far too noisy to use as is
  + bb/100 literally represents how much a player wins over 100 hands.  Yet my model is treating players with 100k hands the same as those with 2500 hands
  + Use the central limit theorem to determine confidence intervals for bb/100 treating each 100 hands as an individual sample.  Take winners and losers and attempt to run a logistic model to classify these players.  
    +  In plain text,"Create a population with 95% confidence intervals that don't contain bb/100=0 " 
  
## Develop Confidence Intervals for Winrate
+ Conditions for inference 
  +  N needs to be at least 30. 
    + Using the 2500 hand filter is close enough to meeting this requirement(minimum n is 25).
+ I will assume the population mean is normal and use a Z score
    + This is likely a poor assumption.  Our BB/100 distribution has a long tail.
+ Observations should be independent of each other-Check
+ One major issue with this attempt is that I don't have SD for each observation, only the SD for the sample mean of winrate.  
    + The best idea I could come up with here, was to just assign SD logically
        + Players with higher vpip tend to swing more, therefore they have higher SD
        + The typical range for the SD of winrate, from reports that have the information, is around 80-100 bb/100.  
        +  Create some intervals 
            + vpip<10 will be assigned a SD of 60
            + 10<vpip<20 will be assigned a SD of 70
            + 20<vpip<30 will be assigned a SD of 80
            + 30<vpip<40 will be assigned a SD of 90
            + 40<vpip<100 will be assigned a range of 110

```{r}
## Create db
db_for_ci <- all_players_finished_db %>% filter(., total_hands > 2500)

## CUT vpip column and create sd column
db_for_ci$estimated_sd <- as.numeric(as.character(cut(db_for_ci$vpip, breaks = c(0, 
    10, 20, 30, 40, Inf), labels = c(60, 70, 80, 90, 110))))


## Create vectors to s-store sd(sample), n-samples, a-sample means point
## Estimate
sample_parameters <- describe(db_for_ci$bb_per_100)
s <- db_for_ci$estimated_sd
n <- db_for_ci$total_hands/100
a <- db_for_ci$bb_per_100

## Make confidence intervals
names <- as.character(db_for_ci$playername)
left <- round(a - qnorm(0.975) * s/sqrt(n), 2)
right <- round(a + qnorm(0.975) * s/sqrt(n), 2)

## Build df of overall intervals
df_with_intervals <- as_data_frame(cbind(names, left, right))
kable(head(df_with_intervals))
df_with_intervals[, 2:3] <- lapply(df_with_intervals[, 2:3], function(x)
{
    as.numeric(x)
})


## Run the hpyothesis test and create a vector of names
confidence_intervals <- df_with_intervals %>% filter(., (right < 0 & left < 
    0) | (right > 0 & left > 0))

colnames(confidence_intervals) <- c("playername", "left", "right")
confidence_intervals <- plyr::join(confidence_intervals, db_for_ci)
my_names_2 <- confidence_intervals$playername
## Cut a winner loser category into df
confidence_intervals$winner_loser <- cut(confidence_intervals$left, breaks = c(-Inf, 
    0, Inf), labels = c(0, 1))
confidence_intervals$winner_loser
```

+ After filtering for winners/losers, 287 samples remain from original 1080

## Last Linear Model Before Logistic Regression 
+ Rewrite code from earlier to create a vpip-pfr
+ Run fit_7 
```{r}

# Create numeric column
confidence_intervals <- confidence_intervals %>% mutate(., vpip_pfr = vpip - 
    pfr)
# Create categorical factor column
confidence_intervals$vpip_pfr[confidence_intervals$vpip_pfr < 15.001] <- 1
confidence_intervals$vpip_pfr[confidence_intervals$vpip_pfr > 15.001] <- 0
my_vector <- str_replace(as.character(confidence_intervals$vpip_pfr), "0", 
    "wide_gap")
my_vector <- str_replace(my_vector, "1", "narrow_gap")
confidence_intervals$vpip_pfr <- as_factor(my_vector)
## Run LM model fit_7
confidence_intervals <- confidence_intervals %>% filter(., playername %in% 
    my_names_2) %>% select(c(vpip_pfr, wwsf, vpip, pfr, threebet, vpip_pfr, 
    bb_per_100, winner_loser))
y <- confidence_intervals$bb_per_100
vpip_pfr <- confidence_intervals$vpip_pfr
WWSF <- confidence_intervals$wwsf
VPIP <- confidence_intervals$vpip
PFR <- confidence_intervals$pfr
THREE_BET <- confidence_intervals$threebet
WIN_LOSE <- confidence_intervals$winner_loser
fit_7 <- lm(y ~ vpip_pfr + VPIP + THREE_BET + PFR + WIN_LOSE)
layout(matrix(c(1, 2, 3, 4), 2, 2))
plot(fit_7)
summary(fit_7)
anova(fit_7)
```

## Run RMSE 
```{r}
set.seed(20)
train.idx <- sample(nrow(confidence_intervals), 0.7 * nrow(confidence_intervals), 
    replace = FALSE)
test.idx <- (1:nrow(confidence_intervals))[-train.idx]
## Extract target vector and rest of DF for LM
lm_target <- confidence_intervals[, "bb_per_100"]
lm_inputs <- confidence_intervals[, c("pfr", "threebet", "vpip_pfr")]
train_df <- confidence_intervals[train.idx, c("pfr", "winner_loser", "threebet", 
    "vpip_pfr", "bb_per_100", "wwsf", "vpip")]
test_df <- confidence_intervals[test.idx, c("pfr", "winner_loser", "threebet", 
    "vpip_pfr", "bb_per_100", "wwsf", "vpip")]
fit_7 <- lm(bb_per_100 ~ I(threebet^2) + I(pfr^2) + I(vpip - pfr) + vpip + 
    wwsf + winner_loser, data = train_df)
summary(fit_7)
fit_7 <- predict(fit_7, test_df)
actual <- confidence_intervals[test.idx, c("bb_per_100")]
error <- actual - fit_7
paste("my RMSE is", sqrt(mean(error^2)))

```

+ That didn't work either, the RMSE is still terrible
+ Last attempt, I will try and use logistic regression and determine winner/loser instead of bb/100

## Logistic Regression

```{r}
train_df <- confidence_intervals[train.idx, c("pfr", "winner_loser", "threebet", 
    "vpip_pfr", "wwsf", "vpip")]
test_df <- confidence_intervals[test.idx, c("pfr", "winner_loser", "threebet", 
    "vpip_pfr", "wwsf", "vpip")]
model <- glm(winner_loser ~ pfr + threebet + vpip_pfr, family = binomial(link = "logit"), 
    data = train_df)
summary(model)
# Use model to make predictions
pred <- predict(model, test_df)
# Convert predictions to probabilities
probs <- exp(pred)/(1 + exp(pred))
probs
## Cut so that anything above .5= winner(1) below .5= loser(0)
convert_logistic_for_matrix <- cut(probs, breaks = c(0, 0.5, 1), labels = c(0, 
    1))
accuracy <- table(convert_logistic_for_matrix, test_df[, "winner_loser"])
accuracy
## accuracy rate
sum(diag(accuracy))/sum(accuracy)
```
   
+ It seems as though the logistic regression is much more impressive
+ An accuracy rate of nearly 94% at identifying winning and losing players 
+ Once again the categorical data is doing the heavy lifting and my other variables don't seem statistically significant 


# Conclusion

It appears that using a linear model to predict winrate may be much more difficult than I assumed.  None of the models performed well enough.  However, the logistic regression classification of winners and losers, performed extremely well.  I believe it worked well for the same reason that the linear models may have worked so poorly.  The categorical data I used of wide_gap narrow_gap(vpip_pfr) does a great job at identifying winning versus losing players.  For simple classification, this works great.  However when it comes to predicting, through a Linear Model, the coefficient of 1 *(narrow_gap) is to reductive to have value.  Perhaps if I could have tuned the other variables better or created higher levels of factors for my vpip_pfr input, the linear models could have performed better. Overall it was helpful to dive into regression with a dataset I am  familiar with.  I hope to do some more tuning and to  implement some of the ideas I mentioned above in the near future.    







